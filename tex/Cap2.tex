\chapter{Regresión sobre componentes}

\section{Mínimos cuadrados}

En está sección haremos una revisión del método de mínimos cuadrado. El contenido aquí expuesto se basa en el material didáctico correspondiente a la referencia \cite{minimos} y \cite{Metodos}.

El término mínimos cuadrados describe un enfoque frecuentemente usado para resolver sistemas de ecuaciones sobredeterminados. En lugar de resolver las ecuaciones exactamente, se busca solamente minimizar la suma de los cuadrados de los residuales. 

Sea $x$ una variable independiente y sea $y(x)$ una función desconocida de $x$ que se desea aproximar, luego suponiendo que tenemos m observaciones $(x_{1}, y_{1}),$ $(x_{2},y_{2}), ...,(x_{m}, y_{m})$, donde $y_{i} \approx y(x_{i})$, con $i = 1, 2, .., m$. La idea es modelar $y(x)$ por medio de una combinación de $n$ funciones base $\phi_{1}(x), \phi_{2}(x), ..., \phi_{n}(x)$. En el caso lineal suponemos que la función que se ajusta a los datos es una \textit{combinación lineal} de la forma

\begin{equation} \label{sistema_ecu}
	y(x) = c_{1}\phi_{1}(x_{i}) + c_{2}\phi_{2}(x_{i}) + ... + c_{n}\phi_{n}(x_{i}) \qquad\qquad i=1, 2, .., m
\end{equation}

La expresión \ref{sistema_ecu} constituye un sistema de $m$ ecuaciones con $n$ incógnitas ($c_{1}, c_{2}, ..., c_{n}$). En el ajuste de las curvas el número de funciones base $n$ es generalmente menor que el número de datos $m$, es decir, $m > n$. En forma matricial la condición \ref{sistema_ecu} puede verse como. 

\begin{equation}\label{rep_matriz}
\begin{bmatrix}
\phi_{1}(x_{1}) & \phi_{2}(x_{1}) & \cdots & \phi_{n}(x_{1}) \\
\phi_{1}(x_{2}) & \phi_{2}(x_{2}) & \cdots & \phi_{n}(x_{2}) \\
\vdots & & \ddots & \vdots \\
\phi_{1}(x_{m}) & \phi_{2}(x_{m}) & \cdots & \phi_{n}(x_{m}) 
\end{bmatrix}
\begin{bmatrix}
c_{1}\\
c_{2}\\
\vdots\\
c_{n}
\end{bmatrix}
=
\begin{bmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{bmatrix}
\end{equation} 

A la matriz de este sistema $A = (a_{ij})$ con $a_{ij} = \phi_{j}(x_{i})$ se le denomina \textit{matriz de diseño}. Las funciones base $\phi_{i}(x)$ con $i = 1, ...n$ pueden ser funciones no lineales de $x$, pero los coeficientes y parámetros $c_{j}$ aparecen en el modelo en la forma lineal cuando se trata de un ajuste lineal. Dependiendo del problema particular y el objetivo de estudio, las funciones base $\phi_{i}(x)$ pueden escogerse de muchas maneras, e incluso pueden depender de ciertos parámetros. Algunas elecciones comunes pueden ser, entre otras: polinomios, $\phi_{j}(x) = x^{j-1}$, funciones racionales, $\phi_{j}(x) = x_{j-1}/(\alpha_{0} + \alpha_{1}x + ... + \alpha_{n-1}x^n-1)$, con $\alpha_{0}, ..., \alpha_{n-1}$ parámetros dados; exponenciales, $\phi_{j}(x) = e^{-\lambda_{j}x}$, con parámetros de decaimiento $\lambda_{i}$.

Dado que $m>n$, el sistema $Ac = y$ dado por \ref{rep_matriz} es sobredeterminado, y por lo tanto, tiene una solución solo si el vector de datos $y$ se encuentra en el espacio de la imagen de $A$, denotado por $Im(A)$ y por lo tanto no es posible encontrar una solución $c$ del sistema \ref{rep_matriz}. Entonces el problema es buscar los coeficientes de la función \ref{sistema_ecu} que mejor ajusten los datos. El enfoque de mínimos cuadrados consiste en buscar aquel vector de coeficientes $c$ que minimice el residual $r = y - Ac$. Si denotamos la norma Euclidiana en $\mathbb{R}^{m}$ por $||\cdot||$, entonces el problema consiste en resolver 

\begin{equation}
\mathop{m\acute{i}n}_{c \in \mathbb{R}^{n}} ||Ac -y||^{2}.
\end{equation} 

Es decir, para encontrar el ajuste de mínimos cuadrados debemos encontrar el vector de coeficientes $c = (c_{1}, ..., c_{n})^{T}$ que minimiza la suma de cuadrados: 

\begin{equation}
\mathop{m\acute{i}n}_{c \in \mathbb{R}^{n}} \sum_{i=1}^{m}(c_{1}\phi_{1}(x_{i}) + c_{2}\phi_{2}(x_{i}) + \cdots + c_{n}\phi_{1}(x_{n})- y_{i})^{2}
\end{equation}

Como vimos en la sección 1.2 la solución de mínimos cudrados se obtiene resolvindo el sistema $(AA^{T})\theta = A^{T}b$.

Ahora presentaremos un ejemplo de un ajuste haciendo uso de este método. Para empezar generamos dos conjuntos de datos, el primer conjunto $(X)$ corresponde al conjunto de variables independientes y son los números del -10 al 9. El segundo conjunto ($Y$) corresponde al conjunto de variables dependientes y es una función cuadrática de $X$ más ruido, es decir $y_{i} = x_{i} + \epsilon$, donde $i = 1, 2,..., 10$ y $\epsilon$ es un número aleatorio entre 1 y 10. Obteniedo así, los conjuntos de datos : 

\begin{center}
\begin{tabular}{|c|c| }
\hline
x & y \\
\hline
-10 & 109 \\
\hline
-9 & 84 \\
\hline
-8 & 71 \\ 
\hline  
-7 & 53 \\
\hline
-6 & 41 \\ 
\hline
-5 & 34 \\ 
\hline 
-4 & 19 \\ 
\hline 
-3 & 15 \\ 
\hline 
-2 & 14 \\
\hline   
-1 & 4 \\  
\hline 
0 & 10 \\ 
\hline   
1 & 9 \\
\hline
2 & 14 \\ 
\hline  
3 & 10 \\ 
\hline  
4 & 24 \\ 
\hline  
5 & 30 \\ 
\hline  
6 & 43 \\ 
\hline  
7 & 51 \\ 
\hline  
8 & 72 \\ 
\hline  
9 & 89 \\
\hline
\end{tabular}
\end{center}


En la imagen \ref{gpo} se observa la gráfica de los datos anteriores y como es que se distrubuyen de una manera casi cuadrática.

\begin{figure}

\centering
\includegraphics[scale=.5]{gmcp.png}
\caption{Gráfica de los puntos generados.}\label{gpo}

\end{figure}

Ahora queremos ajustar una función del tipo $y = \theta_{1} + \theta_{2}x + \theta_{3}x^{2}$ así que la matriz de diseño A queda de la siguiente manera: 
	
\begin{center}
$ A = 
\begin{bmatrix}
1 & -10 & 100 \\
1 & -9 & 81 \\
1 & -8 & 64 \\
\vdots & \vdots & \vdots \\
1 & 0 & 0 \\
\vdots & \vdots & \vdots \\
1 & 9 & 81 \\
\end{bmatrix}
$
\end{center}

Posteriormente a partir del del sistema $(AA^{T})\theta = A^{T}b$, con $b = Y$ y $\theta = (\theta_{1}, \theta_{2}, \theta_{3})^{T}$. El vector $\theta$ o vector de soluciones se obtiene resolviendo el sistema de la siguinete manera: 

\begin{center}
$\theta = (A^{T}A)^{-1}(A^{T}b)$
\end{center}

El vector de soluciones que calculamos en python es $\theta = (6.4, 0.01 ,0.99)^{T} $ y en la gráfica \ref{ajuste} se observa la curva que ajustamos con mínimos cuadrados.

\begin{figure}

\centering
\includegraphics[scale=.5]{ajuste.png}
\caption{Curva de ajuste calculada con mínimos cuadrados.}\label{ajuste}

\end{figure}
	

El código en python para generar los conjuntos de datos mencionados anteriormente y para realizar el ajuste con mínimos cuadrados se presenta en el Apéndice \ref{Cminimos}.
  
\section{Análisis de Componentes principales}

El materia de esta sección revisará el Análisis de Componentes principales y presenta una versión resumida de las fuentes \fix{poner fuentes}.

Cuando se colecta información de una muestra de datos, lo más común es tomar en cuenta el mayor número posible de variables con el fin de que tener una buena descripción de nuestros datos. Sin embargo, si tomamos demasiadas variables tendremos que considerar un número elevado de coeficientes de regresión, dificultando las visualización de las relaciones entre las variables. Otro problema que existe cuando tenemos demasiadas variables es la fuerte correlación que existe en las variables. Es decir, si una variable resulta de la combinación lineal de otras dos, podríamos prescindir de ella. Con lo anterior vemos que es necesario y factible reducir el número de variables. 

El análisis de componentes principales (PCA por sus siglas en inglés) es un método que permite reducir las dimensiones de un conjunto de datos, conservando, en la medida de lo posible, la variación presente en las variables que lo componen, perdiendo así, la menor cantidad posble de información útil. 

La idea principal es la siguiente: dado un conjunto de datos \textbf{X} de dimensión $n \times m$ (n observaciones de m variables cada una) se analiza si es posible representar adecuadamente está información con un número menor de variables llamadas componentes principales, los cuales tienen las características siguientes: 

\begin{itemize}
\item Están construidos como combinaciones lineales de las variables originales. 
\item Están ordenados de tal manera que unos pocos de los primeros retienen la mayor variación presente en las variables originales.
\item El número de componentes principales es inferior a la menor de las dimensiones de \textbf{X}.\\
\end{itemize}

Se considera una serie de variables $(x_{1},x_{2}, ..., x_{n})$ sobre un grupo de objetos o individuos y se trata de calcular, a partir de ellas, un nuevo conjunto de variables $(y_{1}, y_{2}, ..., y_{n})$, incorreladas entre sí, cuyas varianzas vayan decreciendo progresivamente.

Cada $y_{j}$, con  $j=1,2, ...,n$ es una combinación lineal de las $x_{1},x_{2}, ..., x_{n}$ originales, es decir:

\begin{equation}
y_{j} = a_{j1}x_{1} + a_{j2}x_{2} + ... + a_{jn}x_{n} = \mathbf{a^{T}_{j}x} 
\nonumber
\end{equation}

donde $\mathbf{a}^{T}_{j} = (a_{1j}, a_{2j}, ..., a_{nj})$ es un vector de constantes y

\begin{equation}
\mathbf{x} =
\begin{bmatrix}
x_{1} \\
\vdots \\
x_{n}
\end{bmatrix}
\nonumber
\end{equation}

Si lo que se desea es maximizar la varianza de $Y$, una forma simple de logarrlo sería aumentar los coeficientes $a_{ij}$. Por ello, para mantener la ortogonalidad de la transformación se impone que el módulo del vector $\mathbf{a}^{T}_{j} = (a_{1j}, a_{2j}, ..., a_{nj})$ sea 1, es decir

\begin{equation}
\mathbf{a_{j}}^{T} \mathbf{a_{j}} = \sum_{k=1}^{p}a_{kj}^{2} = 1
\end{equation}

El primer componente se calcula eligiendo $\mathbf{a_{1}}$ de modo que $y_{1}$ tenga la mayor varianza posible, sujeta a la restricción de que $\mathbf{a_{j}}^{T} \mathbf{a_{j}} = 1$. El segundo componente principal se calcula obteniendo $\mathbf{a_{2}}$ de modo que la variable obtenida, $y_{2}$ esté incorrelada con $y_{1}$.

Del mismo modo que se eligen $y_{1}, y_{2}, ..., y_{n}$, incorreladas entre sí, de manera que las variables aleatorias obtenidas vayan teniendo cada vez menor varianza.

Existen diferentes maneras de realizar PCA, a continuación explicaremos una de las más utilizadas.

Una manera de realizar PCA es haciendo uso de la \textbf{Descomposición en valores singulares (SVD}, por sus siglas en inglés) de la matriz \textbf{X}. El primer paso es verificar que la matriz \textbf{X} este centrada a media cero, de no ser así, se realizan las operaciones necesarias para lograr lo anterior. Más delante se darán los detalles del algoritmo, por el momento nos limitaremos a describir los pasos y motivar el procedimiento. Una vez que la matriz \textbf{X} esta centrada a media cero se realiza la descomposición en valores singulares de la misma, como se muestra a continuación: 

\begin{center}
	\textbf{SVD(X)} = $U \Delta V ^\top$ 
\end{center}

donde: \\ \\

\noindent \textbf{$U:$ } Eigen-vectores de la matriz $XX^\top.$ $U \in \mathbb{R}^{n\times n}$ \\ \\
\textbf{$\Delta:$ }Matriz asociada a los eigen-valores de las matrices $X^\top X$ y $XX^\top$. $\Delta \in \mathbb{R}^{n\times m} $ y es una matriz diagonal.\\ \\
\textbf{$V^\top:$ }Eigen-vectores de la matriz $X^\top X$. $V^\top \in \mathbb{R}^{m\times m}$.

El siguiente paso es analizar la matriz $\Delta$ para decidir el número de componentes principales que se usarán para describir a la matriz original. Además de que está matriz es diagonal tiene la propiedad de que sus valores están en orden descendente, lo cual indica el grado de variación que conserva cada uno de los componentes principales. Con el conocimiento de lo anterior decidimos cuantos de estos números ayudan a retener la mayor variación posible y definimos \textbf{ncp} (número de componentes pricipales).

El código en Python para realizar PCA sobre un conjunto de datos $X$ de dimendiones $(m \times n)$, es decir $m$ muestras de $m$ variables cada una se puede consultar en el apéndice \ref{Cpca} 

\section{Regresión con componentes principales}

Ahora veremos como combinar la reducción de componentes con el método de mínimos cuadrados con la finalidad de hacer más eficiente y robusto el cálculo de los coeficientes de regresión. 

Asumamos $\mathbb{X}$ al conjunto de variables predictoras y $\mathbb{Y}$ el conjunto de variables respuesta. De tal forma que para cada $x$ tenemos una $y$ correspondiente y consideramos que tanto $x \in \mathbb{X}$ como $y \in \mathbb{Y}$ son de dimensión grande, digamos n y k respectivamente. Luego las $m$ observaciones de $x$ y $y$ se podrían escribir como las siguientes matrices: 

$
\mathbb{X} =
\begin{bmatrix}
 x_{11} & x_{12} & \cdots & x_{1n} \\
 x_{21} & x_{22} & \cdots & x_{2n} \\
 x_{11} & x_{12} & \cdots & x_{1n} \\
 \vdots & \vdots & \ddots & \vdots \\
 x_{m1} & x_{m2} & \cdots & x_{mn}	
\end{bmatrix} 
\qquad\qquad
\mathbb{Y} =
\begin{bmatrix}
 y_{11} & y_{12} & \cdots & y_{1k} \\
 y_{21} & y_{22} & \cdots & y_{2k} \\
 y_{11} & y_{12} & \cdots & y_{1k} \\
 \vdots & \vdots & \ddots & \vdots \\
 y_{m1} & y_{m2} & \cdots & y_{mk}	
\end{bmatrix} $ 
 
Ahora, supongamos que se desea realizar un ajuste por mínimos cuadrados de $\mathbb{Y}$ como combinación lineal de las variables contenidas en $\mathbb{X}$. Un problema que enfrentaremos será la gran dimensión de $\mathbb{X}$ y $\mathbb{Y}$ y las correlaciones que entre las variables existan, es decir, si algunas columnas de $\mathbb{X}$ se pudieran explicar en términos algunas otros columnas de $\mathbb{X}$, entonces podríamos y deberíamos prescindir de algunas columnas de $\mathbb{X}$, igualmente las columnas de $\mathbb{Y}$ que sean combinaciones lineales de otras no tendríamos que predecirlas. Por lo que se debe reducir las dimensiones de ambas variables previamente a calcular la regresión. Este razonamiento, aunque lógico, tiene una débilidad, la cual analizaremos más delante, por lo pronto continuaremos desarrollando está idea. 

El desarrollo del método anteriormente descrito lo haremos en forma visual e iremos introduciendo al lector paso por paso y concluiremos al final con un ejemplo donde mostraremos cual es la deficiencia que se comentó.

Lo primero es realizar una reducción de las dimensiones de $\mathbb{X}$ y $\mathbb{Y}$ haciendo uso de PCA como se muesta en la imagen \ref{regresionPCA}, obteniéndose: 
 
\begin{figure}
\centering
\includegraphics[scale=.5]{regresionPCA.png}
\caption{Etapa de entrenamiento.}\label{regresionPCA}
\end{figure}
 
$
\mathbb{X'} =
\begin{bmatrix}
 x_{11} & x_{12} & \cdots & x_{1n'} \\
 x_{21} & x_{22} & \cdots & x_{2n'} \\
 x_{11} & x_{12} & \cdots & x_{1n'} \\
 \vdots & \vdots & \ddots & \vdots \\
 x_{m1} & x_{m2} & \cdots & x_{mn'}	
\end{bmatrix} 
\qquad
\mathbb{Y'} =
\begin{bmatrix}
 y_{11} & y_{12} & \cdots & y_{1k'} \\
 y_{21} & y_{22} & \cdots & y_{2k'} \\
 y_{11} & y_{12} & \cdots & y_{1k'} \\
 \vdots & \vdots & \ddots & \vdots \\
 y_{m1} & y_{m2} & \cdots & y_{mk'}	
\end{bmatrix} $ 

sabiendo que $k' < k$ y que $n' < n$. 

Una vez que tenemos $\mathbb{X'}$ y $\mathbb{Y'}$ debemos realizar un ajuste por mínimos cuadrados para calcular $\mathbb{Y'}$ a partir de $\mathbb{X'}$, es decir debemos encontrar el valor de $\theta$ resolviendo $\mathop{m\acute{i}n}_{\theta \in \mathbb{R}^{n}} ||A'\theta - \mathbb{Y'}||^{2}$ .

A la etapa anterior se le conoce como etapa de entrenamiento o aprendizaje. Si posteriormente tengo un conjunto de datos no observados ($\mathbb{NY}$) a predecir, es decir, tengo nuevos valores de $\mathbb{X}$ ($\mathbb{NX}$), para realizar la predicción de las $y's$ correspondientes a $\mathbb{NX}$ tendríamos que reducir la demensión de $NX$ al tamoño de $\mathbb{X'}$ obteniendo así $\mathbb{NX'}$ y posteriormente aplicar el ajuste de mínimos cuadrado obtenido en la etapa de entrenamiento para predecir $\mathbb{NY'}$. Por último, una vez que obtuvimos $NY'$ sabemos que esta está en la dimensión del espacio de los PCA's, es decir es de dimensión $m x n'$ y queremos regresarla a su espacio original de $\mathbb{Y}$, es decir, que sea de dimensión $m x n$. Lo anterior se observa en la imagen \ref{regresionPCA}.

\begin{figure}
\centering
\includegraphics[scale=.5]{rpcvno.png}
\caption{Predicción de variables no observadas usando PCA.}\label{regresionPCA}
\end{figure}

El procedimiento anteriormente descrito se describe matemáticamente en los siguientes pasos: 

\begin{enumerate}
\item Cálculo de $n'$ y $k'$ (componentes principales) \\
	$U_{x}\delta_{x}V_{x}^{T} = SVD(\mathbb{X})$\\
	$U_{y}\delta_{y}V_{y}^{T} = SVD(\mathbb{Y})$

\item Reducción de dimensionalidad\\
	$\mathbb{X'} = U_{x}\mathbb{X}$\\
	$\mathbb{Y'} = U_{y}\mathbb{Y}$\\
	$\mathbb{NX'} = U_{x} N\mathbb{X}$

\item Cálculo de parámetros de regresión\\
	$\mathop{m\acute{i}n}_{\theta \in \mathbb{R}^{n}} ||A'\theta -y'||^{2}$

\item Predicción de datos no observados\\
	$\mathbb{NY'} = \theta(\mathbb{NX'})$
	
\item Regreso a dimensión original\\ 
	$\mathbb{NY} = U_{x}^{T}\mathbb{NY'} $

\end{enumerate}

Al realizar el procedimiento podemos entoncotrarnos con el siguiente problema: cabe la posibilidad de que al reducir las demensiones tatdo de $\mathbb{X}$ como de $\mathbb{Y}$ (haciendo un análisis de componentes principales en cada caso) se dejen fuera los elementos que permitan hacer una buena predicción de $\mathbb{Y}$ a partir de $\mathbb{X}$. por lo que necesitamos un método que nos ayude a consevar no los componentes principales, si no los más correlacionados entre las variables $X$ y $Y$. Este método ya existe y se llama Regresión de Mínimos Cuadrados Parciales (PLS, por sus siglas en inglés) y lo veremos en la siguiente sección. 

\section{Regesión de mínimos cuadrados parciales}

La regresión de mínimos cuadrado parciales (PLS, por sus siglas en inglés) es una técnica reciente que combina las características de del análisis de componentes principales y la regresión múltiple. Su principal objetivo es predecir o analizar un conjunto de variables dependientes a partir de un conjunto de variables dependientes o predictores. Esta predicción se logra extrayendo de los predictores un conjunto de factores ortogonales llamados varibles latentes que tiene el mayor valor de predicción. Es un método estadístico que tiene relación con la regresión de componentes principales. Esté método se utiliza para encontrar las relaciones fundamentales entre las matrices $X$ y $Y$.

La regresión de mínimos cuadrados parciales es particular útil cuando necesitamos predecir un conjunto de variables a partir de un conjunto grande de variables independientes (predictores).  



